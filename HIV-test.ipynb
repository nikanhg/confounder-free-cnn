{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper's description about the HIV Dataset\n",
    "\n",
    "### Study Participants\n",
    "- **Age Range**: 18 to 86 years.\n",
    "- **Imaging Technique**: All participants were scanned using a T1-weighted MRI.\n",
    "- **Consent & Approval**:\n",
    "  - Written informed consent was obtained from all study participants.\n",
    "  - The study was approved by the Institutional Review Board (IRB) at Stanford University (Protocol ID: IRB-9861) and SRI International (Protocol ID: Pro00039132).\n",
    "\n",
    "### Participant Details\n",
    "- **HIV Subjects**:\n",
    "  - All were seropositive for HIV infection.\n",
    "  - CD4 count was greater than 100 cells/μL (average: 303.0).\n",
    "\n",
    "### Data Matching and Preprocessing\n",
    "- **Construction of Confounder-independent Subset**:\n",
    "  - Utilized a matching algorithm to extract the maximum number of subjects from each group, ensuring equal size and identical distribution with respect to confounder values.\n",
    "  - For each HIV subject, a control subject was selected based on minimal age difference, continuing until all subjects were matched or the two-tailed p-value of the two-sample t-test between age distributions reached 0.5.\n",
    "- **MRI Preprocessing Steps**:\n",
    "  - Denoising, bias-field correction, skull stripping, and affine registration to the SRI24 template.\n",
    "  - The registered images were then downsampled to a 64 × 64 × 64 volume to reduce potential overfitting and enable large batch sizes during training.\n",
    "\n",
    "### Model Training and Evaluation\n",
    "- **Data Augmentation**:\n",
    "  - New synthetic 3D images were generated by randomly shifting each MRI within one voxel and rotating within 1 degree along the three axes.\n",
    "  - The augmented dataset included a balanced set of 1024 MRIs for each group (control and HIV).\n",
    "- **Assumption**: HIV affects the brain bilaterally; thus, the left hemisphere was flipped to create a second right hemisphere.\n",
    "- **Testing Approach**:\n",
    "  - During testing, both the right and flipped left hemispheres of the raw test images were analyzed by the trained model.\n",
    "  - The prediction score averaged across both hemispheres was used to predict the individual’s diagnosis group.\n",
    "- **Saliency Mapping**:\n",
    "  - Computed for the right hemisphere of each test image to quantify the importance of each voxel to the final prediction.\n",
    "\n",
    "### Cross-validation Strategy\n",
    "- Prediction accuracy of the deep learning models was determined via fivefold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Flatten, UpSampling3D, Input, ZeroPadding3D, Lambda, Reshape\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.constraints import unit_norm, max_norm\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import glob \n",
    "\n",
    "import dcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation:\n",
    "-  In machine learning, particularly in scenarios with limited data (common in medical imaging due to privacy issues, cost, etc.), augmentation is a critical technique to artificially expand the training dataset. This helps prevent overfitting and allows the model to generalize better on unseen data.\n",
    "- Model Robustness: By introducing variability (through rotations and shifts), the function ensures that the neural network becomes robust to such variations in the input data, which is crucial for medical diagnostics where input data can vary significantly in orientation and positioning.\n",
    "- Efficiency: This method of augmentation is computationally cheaper and quicker than acquiring new real-world data, making it an efficient strategy in data-scarce environments like medical imaging.\n",
    "- This augmentation function supports the overall goal of enhancing model training by providing a diverse set of training examples from a limited set of actual samples, thus aiding in developing a more effective and robust machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_by_transformation(data,age,sex,n):\n",
    "    augment_scale = 1\n",
    "\n",
    "    if n <= data.shape[0]:\n",
    "        return data\n",
    "    else:\n",
    "        raw_n = data.shape[0]\n",
    "        m = n - raw_n\n",
    "        new_data = np.zeros((m,data.shape[1],data.shape[2],data.shape[3],1))\n",
    "        for i in range(0,m):\n",
    "            idx = np.random.randint(0,raw_n)\n",
    "            new_age = age[idx]\n",
    "            new_sex = sex[idx]\n",
    "            new_data[i] = data[idx].copy()\n",
    "            new_data[i,:,:,:,0] = sp.ndimage.interpolation.rotate(new_data[i,:,:,:,0],np.random.uniform(-0.5,0.5),axes=(1,0),reshape=False)\n",
    "            new_data[i,:,:,:,0] = sp.ndimage.interpolation.rotate(new_data[i,:,:,:,0],np.random.uniform(-0.5,0.5),axes=(0,2),reshape=False)\n",
    "            new_data[i,:,:,:,0] = sp.ndimage.interpolation.rotate(new_data[i,:,:,:,0],np.random.uniform(-0.5,0.5),axes=(1,2),reshape=False)\n",
    "            new_data[i,:,:,:,0] = sp.ndimage.shift(new_data[i,:,:,:,0],np.random.uniform(-0.5,0.5))\n",
    "\n",
    "            age = np.append(age, new_age)\n",
    "            sex = np.append(sex, new_sex)\n",
    "\n",
    "        # output an example\n",
    "        array_img = nib.Nifti1Image(np.squeeze(new_data[3,:,:,:,0]),np.diag([1, 1, 1, 1]))  \n",
    "        filename = 'augmented_example.nii.gz'\n",
    "        nib.save(array_img,filename)\n",
    "\n",
    "        data = np.concatenate((data, new_data), axis=0)\n",
    "        return data,age,sex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. inv_mse (Inverse Mean Squared Error)\n",
    "\n",
    "- This function computes the mean squared error (MSE), which is a common measure of the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. Here, it is calculated as the sum of squared differences between y_true and y_pred. Uniquely, this function returns the negative of the MSE.\n",
    "\n",
    "- Returning the negative of the MSE could be used for scenarios where one might need to maximize MSE, possibly in adversarial settings or specific optimization scenarios where the model aims to diverge from a particular solution. It's an unusual application, as typically MSE is minimized.\n",
    "2. inv_correlation_coefficient_loss\n",
    "\n",
    "- This function computes a variation of the Pearson correlation coefficient between the true and predicted values. The Pearson correlation assesses the linear relationship between two datasets. Standard Pearson's r ranges from -1 to +1, where +1 indicates total positive linear correlation, 0 indicates no linear correlation, and -1 indicates total negative linear correlation. This specific implementation squares the correlation coefficient and subtracts it from 1, effectively reversing its effect.\n",
    "\n",
    "- This loss is likely designed to minimize correlation between predictions and actuals, potentially useful in scenarios where independence between outputs and true values is desired. This could be useful in regularization or in designing features that should not correlate with the noise or undesired signals in the data.\n",
    "3. correlation_coefficient_loss\n",
    "\n",
    "- Similar to inv_correlation_coefficient_loss, but it directly returns the square of the Pearson correlation coefficient. This version emphasizes promoting higher correlation between the predicted and true values.\n",
    "\n",
    "- This loss function is used when you want to maximize the correlation between the predictions and the actual values. It's suitable for regression problems where the goal is to align as closely as possible with the variability in the data, adjusted linearly.\n",
    "Overall Usage:\n",
    "These loss functions can be selected based on specific training goals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_mse(y_true, y_pred):\n",
    "    mse_value = K.sum(K.square(y_true-y_pred))\n",
    "\n",
    "    return -mse_value\n",
    "\n",
    "def inv_correlation_coefficient_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym)))) + 1e-5\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return 1 - K.square(r)\n",
    "\n",
    "def correlation_coefficient_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym)))) + 1e-5\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return K.square(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For learning GANNs:\n",
    " - https://youtu.be/8L11aMN5KY8?si=41BEjE-QQ0fbbtqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overview of the Code Structure\n",
    "- **Optimizer Setup**: Multiple optimizers are set up, probably to handle different training requirements for each network component.\n",
    "- **Regressor and Encoder**: These parts of the network are responsible for processing the input data and extracting meaningful features. The encoder acts as the feature extractor mentioned in the paper, reducing each medical image to a vector of features.\n",
    "- **Distiller Component**: Although not explicitly named as such in your code, the use of the regressor in a manner that it is not updated during the training of the encoder suggests a role similar to a distillation process where the knowledge is transferred or refined.\n",
    "- **Classifier**: This part uses the features processed by the encoder to make final predictions (e.g., disease presence or absence).\n",
    "\n",
    "### 2. Specific Functions Mapped to Paper Descriptions\n",
    "- **Encoder**: The encoder in your code likely corresponds to the **Feature Extractor (FE)** in the paper. It processes input images into a condensed form of features that are useful for prediction but should ideally be invariant to confounding factors like age or sex.\n",
    "  \n",
    "- **Regressor Setup as Non-Trainable in the Context of Distiller**: The regressor might be akin to the **Confounder Predictor (CP)** described in the paper, although it appears to be used here more for feature transformation or distillation rather than directly predicting the confounder. In the paper, CP is used in an adversarial setting to ensure that the features extracted are independent of confounders.\n",
    "\n",
    "- **Classifier and Workflow Compilation**: This aligns with the **Classifier/Predictor (P)** in the paper, which uses the features provided by the FE to predict the outcome, such as a medical diagnosis, while ideally being free from the influence of confounders.\n",
    "\n",
    "### 3. Handling of Confounders\n",
    "In the context of the paper, the network should be learning to extract features that are informative for the prediction task but invariant to confounding factors (like age or sex differences that are not relevant to the disease being studied). This setup is suggested by the use of specific loss functions and the architecture setup where different parts of the network are optimized to either predict the primary outcome or ensure that the features are not confounded.\n",
    "\n",
    "\n",
    "### 4. Train function\n",
    "This function integrates multiple training and evaluation processes within a single training loop, reflecting a complex workflow typical in medical imaging studies where both diagnostic prediction and confounding factor analysis (like age prediction) are relevant. The function emphasizes robustness and generalization by incorporating data augmentation directly in the training loop and by evaluating models on both original and manipulated datasets. This approach is critical in medical applications where model performance and interpretability directly impact clinical outcomes.\n",
    "\n",
    "### Conclusion\n",
    "Your code seems to implement a sophisticated neural network that attempts to integrate the extraction of useful and confounder-free features from medical images, closely aligning with the objectives discussed in the paper. The actual mechanism by which confounding is addressed (e.g., adversarial training components or specific regularization techniques) would depend on further details from the paper and additional parts of the code not provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "        def __init__(self):\n",
    "                self.lr = 0.0002\n",
    "                optimizer = Adam(self.lr)\n",
    "                optimizer_distiller = Adam(self.lr)\n",
    "                optimizer_regressor = Adam(self.lr)\n",
    "\n",
    "                L2_reg = 0.1\n",
    "                ft_bank_baseline = 16\n",
    "                latent_dim = 16\n",
    "\n",
    "                # Build and compile the cf predictorinv_inv\n",
    "                self.regressor = self.build_regressor()\n",
    "                self.regressor.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "                #The cnn\n",
    "                # Build the feature encoder\n",
    "                input_image = Input(shape=(32,64,64,1), name='input_image')\n",
    "                feature = Conv3D(ft_bank_baseline, activation='relu', kernel_size=(3, 3, 3),padding='same')(input_image)\n",
    "                feature = BatchNormalization()(feature)\n",
    "                feature = MaxPooling3D(pool_size=(2, 2, 2))(feature)\n",
    "\n",
    "                feature = Conv3D(ft_bank_baseline*2, activation='relu', kernel_size=(3, 3, 3),padding='same')(feature)\n",
    "                feature = BatchNormalization()(feature)\n",
    "                feature = MaxPooling3D(pool_size=(2, 2, 2))(feature)\n",
    "\n",
    "                feature = Conv3D(ft_bank_baseline*4, activation='relu', kernel_size=(3, 3, 3),padding='same')(feature)\n",
    "                feature = BatchNormalization()(feature)\n",
    "                feature = MaxPooling3D(pool_size=(2, 2, 2))(feature)\n",
    "\n",
    "                feature = Conv3D(ft_bank_baseline*2, activation='relu', kernel_size=(3, 3, 3),padding='same')(feature)\n",
    "                #feature = Conv3D(ft_bank_baseline*8, activation='relu', kernel_size=(3, 3, 3),padding='same')(feature)\n",
    "                feature = BatchNormalization()(feature)\n",
    "                feature = MaxPooling3D(pool_size=(2, 2, 2))(feature)\n",
    "\n",
    "                feature_dense = Flatten()(feature)\n",
    "\n",
    "                self.encoder = Model(input_image, feature_dense)\n",
    "\n",
    "                # the CF part with regression, we are making it confounder free\n",
    "\n",
    "                # For the distillation model we will only train the encoder\n",
    "\n",
    "                self.regressor.trainable = False\n",
    "                cf = self.regressor(feature_dense)\n",
    "                self.distiller = Model(input_image, cf)\n",
    "                self.distiller.compile(loss=correlation_coefficient_loss, optimizer=optimizer)\n",
    "\n",
    "                # classifier:\n",
    "\n",
    "                # Build and Compile the classifer  \n",
    "                #self.encoder.load_weights('encoder.h5');\n",
    "                #self.encoder.trainable = False\n",
    "                input_feature_clf = Input(shape=(1024,), name='input_feature_dense')\n",
    "                #input_feature_clf = Input(shape=(4096,), name='input_feature_dense')\n",
    "                feature_clf = Dense(latent_dim*4, activation='tanh',kernel_regularizer=regularizers.l2(L2_reg))(input_feature_clf)\n",
    "                feature_clf = Dense(latent_dim*2, activation='tanh',kernel_regularizer=regularizers.l2(L2_reg))(feature_clf)\n",
    "                prediction_score = Dense(1, name='prediction_score',kernel_regularizer=regularizers.l2(L2_reg))(feature_clf)\n",
    "                self.classifier = Model(input_feature_clf, prediction_score)\n",
    "\n",
    "                # workflow and ouput:\n",
    "\n",
    "                # Build the entir workflow\n",
    "                prediction_score_workflow = self.classifier(feature_dense)\n",
    "                label_workflow = Activation('sigmoid', name='r_mean')(prediction_score_workflow)\n",
    "                self.workflow = Model(input_image, label_workflow)\n",
    "                self.workflow.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "#  the counter factual (CF) part, which is the regressor and the input is the encoders output\n",
    "        def build_regressor(self):\n",
    "                latent_dim = 16\n",
    "                inputs_x = Input(shape=(1024,))\n",
    "                #inputs_x = Input(shape=(4096,))\n",
    "                feature = Dense(latent_dim*4, activation='tanh')(inputs_x)\n",
    "                feature = Dense(latent_dim*2, activation='tanh')(feature)\n",
    "                cf = Dense(1)(feature)\n",
    "\n",
    "                return Model(inputs_x, cf)\n",
    "\n",
    "\n",
    "        def train(self, epochs, training, testing, testing_raw, batch_size=64, fold=0):\n",
    "                [train_data_aug, train_dx_aug, train_age_aug, train_sex_aug] = training\n",
    "                [test_data_aug,  test_dx_aug,  test_age_aug,  test_sex_aug]  = testing\n",
    "                [test_data    ,  test_dx    ,  test_age,      test_sex   ]   = testing_raw\n",
    "\n",
    "                test_data_aug_flip = np.flip(test_data_aug,1)\n",
    "                test_data_flip = np.flip(test_data,1)\n",
    "\n",
    "                idx_perm = np.random.permutation(int(train_data_aug.shape[0]/2))\n",
    "\n",
    "                dc_age = np.zeros((int(epochs/10)+1,))\n",
    "                min_dc = 0\n",
    "                for epoch in range(epochs):\n",
    "\n",
    "                        ## Turn on to LR decay manually\n",
    "                        # if epoch % 200 == 0:\n",
    "                        #    self.lr = self.lr * 0.75\n",
    "                        #    optimizer = Adam(self.lr)\n",
    "                        #    self.workflow.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "                        #    self.distiller.compile(loss=correlation_coefficient_loss, optimizer=optimizer)\n",
    "                        #    self.regressor.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "                        # Select a random batch of images\n",
    "                        \n",
    "                        idx_perm = np.random.permutation(int(train_data_aug.shape[0]/2))\n",
    "                        ctrl_idx = idx_perm[:int(batch_size)]\n",
    "                        idx_perm = np.random.permutation(int(train_data_aug.shape[0]/2))\n",
    "                        idx = idx_perm[:int(batch_size/2)]\n",
    "                        idx = np.concatenate((idx,idx+int(train_data_aug.shape[0]/2)))\n",
    "\n",
    "                        training_feature_batch = train_data_aug[idx]\n",
    "                        dx_batch = train_dx_aug[idx]\n",
    "                        age_batch = train_age_aug[idx]\n",
    "\n",
    "                        training_feature_ctrl_batch = train_data_aug[ctrl_idx]\n",
    "                        age_ctrl_batch = train_age_aug[ctrl_idx]\n",
    "                        \n",
    "                        # ---------------------\n",
    "                        #  Train regressor (cf predictor)\n",
    "                        # ---------------------\n",
    "\n",
    "                        encoded_feature_ctrl_batch = self.encoder.predict(training_feature_ctrl_batch[:,:32,:,:])\n",
    "                        r_loss = self.regressor.train_on_batch(encoded_feature_ctrl_batch, age_ctrl_batch)\n",
    "\n",
    "                        # ---------------------\n",
    "                        #  Train Disstiller\n",
    "                        # ---------------------\n",
    "                        \n",
    "                        g_loss = self.distiller.train_on_batch(training_feature_ctrl_batch[:,:32,:,:], age_ctrl_batch)\n",
    "                        \n",
    "                        # ---------------------\n",
    "                        #  Train Encoder & Classifier\n",
    "                        # ---------------------\n",
    "                        \n",
    "                        c_loss = self.workflow.train_on_batch(training_feature_batch[:,:32,:,:], dx_batch)\n",
    "\n",
    "                        # ---------------------\n",
    "                        #  flip & re-do everything\n",
    "                        # ---------------------\n",
    "\n",
    "                        training_feature_batch = np.flip(training_feature_batch,1)\n",
    "                        training_feature_ctrl_batch = np.flip(training_feature_ctrl_batch,1)\n",
    "\n",
    "                        encoded_feature_ctrl_batch = self.encoder.predict(training_feature_ctrl_batch[:,:32:,:])\n",
    "                        r_loss = self.regressor.train_on_batch(encoded_feature_ctrl_batch, age_ctrl_batch)\n",
    "                        g_loss = self.distiller.train_on_batch(training_feature_ctrl_batch[:,:32,:,:], age_ctrl_batch)\n",
    "                        c_loss = self.workflow.train_on_batch(training_feature_batch[:,:32,:,:], dx_batch)\n",
    "\n",
    "                        # Plot the progress\n",
    "                        if epoch % 50 == 0:\n",
    "                                c_loss_test_1 = self.workflow.evaluate(test_data_aug[:,:32,:,:],      test_dx_aug, verbose = 0, batch_size = batch_size)    \n",
    "                                c_loss_test_2 = self.workflow.evaluate(test_data_aug_flip[:,:32,:,:], test_dx_aug, verbose = 0, batch_size = batch_size)    \n",
    "\n",
    "                                # feature dist corr\n",
    "                                features_dense = self.encoder.predict(train_data_aug[train_dx_aug == 0,:32,:,:],  batch_size = batch_size)\n",
    "                                dc_age[int(epoch/10)] = dcor.u_distance_correlation_sqr(features_dense, train_age_aug[train_dx_aug == 0])\n",
    "                                print (\"%d [Acc: %f,  Test Acc: %f %f,  dc: %f]\" % (epoch, c_loss[1], c_loss_test_1[1], c_loss_test_2[1], dc_age[int(epoch/10)]))\n",
    "                                sys.stdout.flush()\n",
    "\n",
    "                                self.classifier.save_weights(\"res_cf_5cv/classifier.h5\")\n",
    "                                self.encoder.save_weights(\"res_cf_5cv/encoder.h5\")\n",
    "                                self.workflow.save_weights(\"res_cf_5cv/workflow.h5\")\n",
    "\n",
    "                                ## Turn on to save all intermediate features for posthoc MI computation\n",
    "                                #features_dense = self.encoder.predict(test_data[:,:32,:,:],  batch_size = 64)\n",
    "                                #filename = 'res_cf/features_'+str(fold)+'.txt'\n",
    "                                #np.savetxt(filename,features_dense)\n",
    "                                #score = self.classifier.predict(features_dense,  batch_size = 64)\n",
    "                                #filename = 'res_cf/scores_'+str(fold)+'_'+str(epoch)+'.txt'\n",
    "                                #np.savetxt(filename,score)\n",
    "\n",
    "                                #features_dense = self.encoder.predict(test_data_flip[:,:32,:,:],  batch_size = 64)\n",
    "                                #filename = 'res_cf/features_flip_'+str(fold)+'.txt'\n",
    "                                #np.savetxt(filename,features_dense)\n",
    "                                #score = self.classifier.predict(features_dense,  batch_size = 64)\n",
    "                                #filename = 'res_cf/scores_flip_'+str(fold)+'_'+str(epoch)+'.txt'\n",
    "                                #np.savetxt(filename,score)\n",
    "\n",
    "                                # save intermediate predictions\n",
    "                                prediction = self.workflow.predict(test_data[:,:32,:,:],  batch_size = 64)\n",
    "                                filename = 'res_cf_5cv/prediction_'+str(fold)+'_'+str(epoch)+'.txt'\n",
    "                                np.savetxt(filename,prediction)\n",
    "                                prediction = self.workflow.predict(test_data_flip[:,:32,:,:],  batch_size = 64)\n",
    "                                filename = 'res_cf_5cv/prediction_flip_'+str(fold)+'_'+str(epoch)+'.txt'\n",
    "                                np.savetxt(filename,prediction)\n",
    "\n",
    "                                # save ground-truth\n",
    "                                filename = 'res_cf_5cv/dx_'+str(fold)+'.txt'\n",
    "                                np.savetxt(filename,test_dx)    \n",
    "                                filename = 'res_cf_5cv/cf_'+str(fold)+'.txt'\n",
    "                                np.savetxt(filename,test_age)  \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "./access.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     file_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./access.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     age \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./age.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m      4\u001b[0m     sex \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./sex.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\x4287225\\Desktop\\confounder-free-cnn\\env\\Lib\\site-packages\\numpy\\lib\\npyio.py:1980\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   1978\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[0;32m   1979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1980\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1981\u001b[0m     fid_ctx \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mclosing(fid)\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\x4287225\\Desktop\\confounder-free-cnn\\env\\Lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\x4287225\\Desktop\\confounder-free-cnn\\env\\Lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: ./access.txt not found."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    file_idx = np.genfromtxt('./access.txt', dtype='str')\n",
    "    age = np.loadtxt('./age.txt') \n",
    "    sex = np.loadtxt('./sex.txt') \n",
    "    dx = np.loadtxt('./dx.txt') \n",
    "\n",
    "    np.random.seed(seed=0)\n",
    "\n",
    "    subject_num = file_idx.shape[0]\n",
    "    patch_x = 64\n",
    "    patch_y = 64\n",
    "    patch_z = 64\n",
    "    min_x = 0 \n",
    "    min_y = 0 \n",
    "    min_z = 0\n",
    "\n",
    "    augment_size = 512\n",
    "    data = np.zeros((subject_num, patch_x, patch_y, patch_z,1))\n",
    "    i = 0\n",
    "    for subject_idx in file_idx:\n",
    "        #subject_string = format(int(subject_idx),'04d')\n",
    "        filename_full = '/fs/neurosci01/qingyuz/lab_data/img_64_longitudinal/'+subject_idx\n",
    "\n",
    "        img = nib.load(filename_full)\n",
    "        img_data = img.get_fdata()\n",
    "\n",
    "        data[i,:,:,:,0] = img_data[min_x:min_x+patch_x, min_y:min_y+patch_y, min_z:min_z+patch_z] \n",
    "        data[i,:,:,:,0] = (data[i,:,:,:,0] - np.mean(data[i,:,:,:,0])) / np.std(data[i,:,:,:,0])\n",
    "\n",
    "        # output an example\n",
    "        array_img = nib.Nifti1Image(np.squeeze(data[i,:,:,:,0]),np.diag([1, 1, 1, 1]))  \n",
    "        filename = 'processed_example.nii.gz'\n",
    "        nib.save(array_img,filename)\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    ## Train on whole dataset\n",
    "    #train_data_pos = data[dx==1];\n",
    "    #train_data_neg = data[dx==0];\n",
    "    #train_age_pos = age[dx==1];\n",
    "    #train_age_neg = age[dx==0];\n",
    "    #train_sex_pos = sex[dx==1];\n",
    "    #train_sex_neg = sex[dx==0];\n",
    "\n",
    "    #train_data_pos_aug,train_age_pos_aug,train_sex_pos_aug = augment_by_transformation(train_data_pos,train_age_pos,train_sex_pos,augment_size)\n",
    "    #del train_data_pos\n",
    "    #train_data_neg_aug,train_age_neg_aug,train_sex_neg_aug = augment_by_transformation(train_data_neg,train_age_neg,train_sex_neg,augment_size)\n",
    "    #del train_data_neg\n",
    "\n",
    "    #train_data_aug = np.concatenate((train_data_neg_aug, train_data_pos_aug), axis=0)\n",
    "    #del train_data_neg_aug\n",
    "    #del train_data_pos_aug\n",
    "    #train_age_aug = np.concatenate((train_age_neg_aug, train_age_pos_aug), axis=0)\n",
    "    #train_sex_aug = np.concatenate((train_sex_neg_aug, train_sex_pos_aug), axis=0)\n",
    "    #train_dx_aug = np.zeros((augment_size * 2,))\n",
    "    #train_dx_aug[augment_size:] = 1   \n",
    "     \n",
    "    #gan = GAN()\n",
    "    #gan.train(epochs=1501, training=[train_data_aug, train_dx_aug, train_age_aug, train_sex_aug], testing=[train_data_aug, train_dx_aug, train_age_aug, train_sex_aug], testing_raw=[data, dx, age, sex], batch_size=64, fold=0)\n",
    "    \n",
    "    #exit()\n",
    "\n",
    "    ## cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "    pred = np.zeros((dx.shape))\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, test_idx in skf.split(data, dx):  \n",
    "        if fold < 3:\n",
    "            fold = fold + 1\n",
    "            continue\n",
    "\n",
    "        train_data = data[train_idx]\n",
    "        train_dx = dx[train_idx]\n",
    "        train_age = age[train_idx]\n",
    "        train_sex = sex[train_idx]\n",
    "\n",
    "        test_data = data[test_idx]\n",
    "        test_dx = dx[test_idx]\n",
    "        test_age = age[test_idx]\n",
    "        test_sex = sex[test_idx]\n",
    "\n",
    "        # augment data\n",
    "        train_data_pos = train_data[train_dx==1];\n",
    "        train_data_neg = train_data[train_dx==0];\n",
    "        train_age_pos = train_age[train_dx==1];\n",
    "        train_age_neg = train_age[train_dx==0];\n",
    "        train_sex_pos = train_sex[train_dx==1];\n",
    "        train_sex_neg = train_sex[train_dx==0];\n",
    "\n",
    "        train_data_pos_aug,train_age_pos_aug,train_sex_pos_aug = augment_by_transformation(train_data_pos,train_age_pos,train_sex_pos,augment_size)\n",
    "        train_data_neg_aug,train_age_neg_aug,train_sex_neg_aug = augment_by_transformation(train_data_neg,train_age_neg,train_sex_neg,augment_size)\n",
    "\n",
    "        train_data_aug = np.concatenate((train_data_neg_aug, train_data_pos_aug), axis=0)\n",
    "        train_age_aug = np.concatenate((train_age_neg_aug, train_age_pos_aug), axis=0)\n",
    "        train_sex_aug = np.concatenate((train_sex_neg_aug, train_sex_pos_aug), axis=0)\n",
    "        train_dx_aug = np.zeros((augment_size * 2,))\n",
    "        train_dx_aug[augment_size:] = 1\n",
    "\n",
    "        test_data_pos = test_data[test_dx==1];\n",
    "        test_data_neg = test_data[test_dx==0];\n",
    "        test_age_pos = test_age[test_dx==1];\n",
    "        test_age_neg = test_age[test_dx==0];\n",
    "        test_sex_pos = test_sex[test_dx==1];\n",
    "        test_sex_neg = test_sex[test_dx==0];\n",
    "\n",
    "        test_data_pos_aug,test_age_pos_aug,test_sex_pos_aug = augment_by_transformation(test_data_pos,test_age_pos,test_sex_pos,500)\n",
    "        test_data_neg_aug,test_age_neg_aug,test_sex_neg_aug = augment_by_transformation(test_data_neg,test_age_neg,test_sex_neg,500)\n",
    "\n",
    "        test_data_aug = np.concatenate((test_data_neg_aug, test_data_pos_aug), axis=0)\n",
    "        test_age_aug = np.concatenate((test_age_neg_aug, test_age_pos_aug), axis=0)\n",
    "        test_sex_aug = np.concatenate((test_sex_neg_aug, test_sex_pos_aug), axis=0)\n",
    "        test_dx_aug = np.zeros((500 * 2,))\n",
    "        test_dx_aug[500:] = 1\n",
    "\n",
    "        print(\"Begin Training fold\")\n",
    "        sys.stdout.flush()\n",
    "        gan = GAN()\n",
    "        gan.train(epochs=1501, training=[train_data_aug, train_dx_aug, train_age_aug, train_sex_aug], testing=[test_data_aug, test_dx_aug, test_age_aug, test_sex_aug], testing_raw=[test_data, test_dx, test_age, test_sex], batch_size=64, fold=fold)\n",
    "        fold = fold + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
