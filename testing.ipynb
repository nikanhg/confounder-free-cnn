{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, UpSampling3D, Input, ZeroPadding3D, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras.constraints import unit_norm, max_norm\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import glob \n",
    "\n",
    "import dcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation:\n",
    "-  In machine learning, particularly in scenarios with limited data (common in medical imaging due to privacy issues, cost, etc.), augmentation is a critical technique to artificially expand the training dataset. This helps prevent overfitting and allows the model to generalize better on unseen data.\n",
    "- Model Robustness: By introducing variability (through rotations and shifts), the function ensures that the neural network becomes robust to such variations in the input data, which is crucial for medical diagnostics where input data can vary significantly in orientation and positioning.\n",
    "- Efficiency: This method of augmentation is computationally cheaper and quicker than acquiring new real-world data, making it an efficient strategy in data-scarce environments like medical imaging.\n",
    "- This augmentation function supports the overall goal of enhancing model training by providing a diverse set of training examples from a limited set of actual samples, thus aiding in developing a more effective and robust machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_by_transformation(data,age,sex,n):\n",
    "    augment_scale = 1\n",
    "\n",
    "    if n <= data.shape[0]:\n",
    "        return data\n",
    "    else:\n",
    "        raw_n = data.shape[0]\n",
    "        m = n - raw_n\n",
    "        new_data = np.zeros((m,data.shape[1],data.shape[2],data.shape[3],1))\n",
    "        for i in range(0,m):\n",
    "            idx = np.random.randint(0,raw_n)\n",
    "            new_age = age[idx]\n",
    "            new_sex = sex[idx]\n",
    "            new_data[i] = data[idx].copy()\n",
    "            new_data[i,:,:,:,0] = sp.ndimage.interpolation.rotate(new_data[i,:,:,:,0],np.random.uniform(-0.5,0.5),axes=(1,0),reshape=False)\n",
    "            new_data[i,:,:,:,0] = sp.ndimage.interpolation.rotate(new_data[i,:,:,:,0],np.random.uniform(-0.5,0.5),axes=(0,2),reshape=False)\n",
    "            new_data[i,:,:,:,0] = sp.ndimage.interpolation.rotate(new_data[i,:,:,:,0],np.random.uniform(-0.5,0.5),axes=(1,2),reshape=False)\n",
    "            new_data[i,:,:,:,0] = sp.ndimage.shift(new_data[i,:,:,:,0],np.random.uniform(-0.5,0.5))\n",
    "\n",
    "            age = np.append(age, new_age)\n",
    "            sex = np.append(sex, new_sex)\n",
    "\n",
    "        # output an example\n",
    "        array_img = nib.Nifti1Image(np.squeeze(new_data[3,:,:,:,0]),np.diag([1, 1, 1, 1]))  \n",
    "        filename = 'augmented_example.nii.gz'\n",
    "        nib.save(array_img,filename)\n",
    "\n",
    "        data = np.concatenate((data, new_data), axis=0)\n",
    "        return data,age,sex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. inv_mse (Inverse Mean Squared Error)\n",
    "\n",
    "- This function computes the mean squared error (MSE), which is a common measure of the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values and the actual value. Here, it is calculated as the sum of squared differences between y_true and y_pred. Uniquely, this function returns the negative of the MSE.\n",
    "\n",
    "- Returning the negative of the MSE could be used for scenarios where one might need to maximize MSE, possibly in adversarial settings or specific optimization scenarios where the model aims to diverge from a particular solution. It's an unusual application, as typically MSE is minimized.\n",
    "2. inv_correlation_coefficient_loss\n",
    "\n",
    "- This function computes a variation of the Pearson correlation coefficient between the true and predicted values. The Pearson correlation assesses the linear relationship between two datasets. Standard Pearson's r ranges from -1 to +1, where +1 indicates total positive linear correlation, 0 indicates no linear correlation, and -1 indicates total negative linear correlation. This specific implementation squares the correlation coefficient and subtracts it from 1, effectively reversing its effect.\n",
    "\n",
    "- This loss is likely designed to minimize correlation between predictions and actuals, potentially useful in scenarios where independence between outputs and true values is desired. This could be useful in regularization or in designing features that should not correlate with the noise or undesired signals in the data.\n",
    "3. correlation_coefficient_loss\n",
    "\n",
    "- Similar to inv_correlation_coefficient_loss, but it directly returns the square of the Pearson correlation coefficient. This version emphasizes promoting higher correlation between the predicted and true values.\n",
    "\n",
    "- This loss function is used when you want to maximize the correlation between the predictions and the actual values. It's suitable for regression problems where the goal is to align as closely as possible with the variability in the data, adjusted linearly.\n",
    "Overall Usage:\n",
    "These loss functions can be selected based on specific training goals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_mse(y_true, y_pred):\n",
    "    mse_value = K.sum(K.square(y_true-y_pred))\n",
    "\n",
    "    return -mse_value\n",
    "\n",
    "def inv_correlation_coefficient_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym)))) + 1e-5\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return 1 - K.square(r)\n",
    "\n",
    "def correlation_coefficient_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym)))) + 1e-5\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return K.square(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For learning GANNs:\n",
    " - https://youtu.be/8L11aMN5KY8?si=41BEjE-QQ0fbbtqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overview of the Code Structure\n",
    "- **Optimizer Setup**: Multiple optimizers are set up, probably to handle different training requirements for each network component.\n",
    "- **Regressor and Encoder**: These parts of the network are responsible for processing the input data and extracting meaningful features. The encoder acts as the feature extractor mentioned in the paper, reducing each medical image to a vector of features.\n",
    "- **Distiller Component**: Although not explicitly named as such in your code, the use of the regressor in a manner that it is not updated during the training of the encoder suggests a role similar to a distillation process where the knowledge is transferred or refined.\n",
    "- **Classifier**: This part uses the features processed by the encoder to make final predictions (e.g., disease presence or absence).\n",
    "\n",
    "### 2. Specific Functions Mapped to Paper Descriptions\n",
    "- **Encoder**: The encoder in your code likely corresponds to the **Feature Extractor (FE)** in the paper. It processes input images into a condensed form of features that are useful for prediction but should ideally be invariant to confounding factors like age or sex.\n",
    "  \n",
    "- **Regressor Setup as Non-Trainable in the Context of Distiller**: The regressor might be akin to the **Confounder Predictor (CP)** described in the paper, although it appears to be used here more for feature transformation or distillation rather than directly predicting the confounder. In the paper, CP is used in an adversarial setting to ensure that the features extracted are independent of confounders.\n",
    "\n",
    "- **Classifier and Workflow Compilation**: This aligns with the **Classifier/Predictor (P)** in the paper, which uses the features provided by the FE to predict the outcome, such as a medical diagnosis, while ideally being free from the influence of confounders.\n",
    "\n",
    "### 3. Handling of Confounders\n",
    "In the context of the paper, the network should be learning to extract features that are informative for the prediction task but invariant to confounding factors (like age or sex differences that are not relevant to the disease being studied). This setup is suggested by the use of specific loss functions and the architecture setup where different parts of the network are optimized to either predict the primary outcome or ensure that the features are not confounded.\n",
    "\n",
    "### Conclusion\n",
    "Your code seems to implement a sophisticated neural network that attempts to integrate the extraction of useful and confounder-free features from medical images, closely aligning with the objectives discussed in the paper. The actual mechanism by which confounding is addressed (e.g., adversarial training components or specific regularization techniques) would depend on further details from the paper and additional parts of the code not provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "        def __init__(self):\n",
    "                self.lr = 0.0002\n",
    "                optimizer = Adam(self.lr)\n",
    "                optimizer_distiller = Adam(self.lr)\n",
    "                optimizer_regressor = Adam(self.lr)\n",
    "\n",
    "                L2_reg = 0.1\n",
    "                ft_bank_baseline = 16\n",
    "                latent_dim = 16\n",
    "\n",
    "                # Build and compile the cf predictorinv_inv\n",
    "                self.regressor = self.build_regressor()\n",
    "                self.regressor.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "                #The cnn\n",
    "                # Build the feature encoder\n",
    "                input_image = Input(shape=(32,64,64,1), name='input_image')\n",
    "                feature = Conv3D(ft_bank_baseline, activation='relu', kernel_size=(3, 3, 3),padding='same')(input_image)\n",
    "                feature = BatchNormalization()(feature)\n",
    "                feature = MaxPooling3D(pool_size=(2, 2, 2))(feature)\n",
    "\n",
    "                feature = Conv3D(ft_bank_baseline*2, activation='relu', kernel_size=(3, 3, 3),padding='same')(feature)\n",
    "                feature = BatchNormalization()(feature)\n",
    "                feature = MaxPooling3D(pool_size=(2, 2, 2))(feature)\n",
    "\n",
    "                feature = Conv3D(ft_bank_baseline*4, activation='relu', kernel_size=(3, 3, 3),padding='same')(feature)\n",
    "                feature = BatchNormalization()(feature)\n",
    "                feature = MaxPooling3D(pool_size=(2, 2, 2))(feature)\n",
    "\n",
    "                feature = Conv3D(ft_bank_baseline*2, activation='relu', kernel_size=(3, 3, 3),padding='same')(feature)\n",
    "                #feature = Conv3D(ft_bank_baseline*8, activation='relu', kernel_size=(3, 3, 3),padding='same')(feature)\n",
    "                feature = BatchNormalization()(feature)\n",
    "                feature = MaxPooling3D(pool_size=(2, 2, 2))(feature)\n",
    "\n",
    "                feature_dense = Flatten()(feature)\n",
    "\n",
    "                self.encoder = Model(input_image, feature_dense)\n",
    "\n",
    "                # the CF part with regression, we are making it confounder free\n",
    "\n",
    "                # For the distillation model we will only train the encoder\n",
    "\n",
    "                self.regressor.trainable = False\n",
    "                cf = self.regressor(feature_dense)\n",
    "                self.distiller = Model(input_image, cf)\n",
    "                self.distiller.compile(loss=correlation_coefficient_loss, optimizer=optimizer)\n",
    "\n",
    "                # classifier:\n",
    "\n",
    "                # Build and Compile the classifer  \n",
    "                #self.encoder.load_weights('encoder.h5');\n",
    "                #self.encoder.trainable = False\n",
    "                input_feature_clf = Input(shape=(1024,), name='input_feature_dense')\n",
    "                #input_feature_clf = Input(shape=(4096,), name='input_feature_dense')\n",
    "                feature_clf = Dense(latent_dim*4, activation='tanh',kernel_regularizer=regularizers.l2(L2_reg))(input_feature_clf)\n",
    "                feature_clf = Dense(latent_dim*2, activation='tanh',kernel_regularizer=regularizers.l2(L2_reg))(feature_clf)\n",
    "                prediction_score = Dense(1, name='prediction_score',kernel_regularizer=regularizers.l2(L2_reg))(feature_clf)\n",
    "                self.classifier = Model(input_feature_clf, prediction_score)\n",
    "\n",
    "                # workflow and ouput:\n",
    "\n",
    "                # Build the entir workflow\n",
    "                prediction_score_workflow = self.classifier(feature_dense)\n",
    "                label_workflow = Activation('sigmoid', name='r_mean')(prediction_score_workflow)\n",
    "                self.workflow = Model(input_image, label_workflow)\n",
    "                self.workflow.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "#  the counter factual (CF) part, which is the regressor and the input is the encoders output\n",
    "        def build_regressor(self):\n",
    "                latent_dim = 16\n",
    "                inputs_x = Input(shape=(1024,))\n",
    "                #inputs_x = Input(shape=(4096,))\n",
    "                feature = Dense(latent_dim*4, activation='tanh')(inputs_x)\n",
    "                feature = Dense(latent_dim*2, activation='tanh')(feature)\n",
    "                cf = Dense(1)(feature)\n",
    "\n",
    "                return Model(inputs_x, cf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
